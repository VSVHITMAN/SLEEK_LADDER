{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.src.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "File = open(\"frankenstein.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# standardization\n",
    "#what is tokenization? Tokenization is the process of breaking a stream of text up into words phrases symbols or some meaningful elements\n",
    "def tokenize_words(input):\n",
    "    input=input.lower()\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    tokens=tokenizer.tokenize(input)\n",
    "    filtered=filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "processed_inputs=tokenize_words(File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "chars=sorted(list(set(processed_inputs)))\n",
    "char_to_num=dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 269566\n",
      "Total vocab: 38\n"
     ]
    }
   ],
   "source": [
    "#check if the words to char or chars to num has worked\n",
    "input_len=len(processed_inputs)\n",
    "vocab_len=len(chars)\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\",vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence length\n",
    "seq_length=100\n",
    "x_data=[]\n",
    "y_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 269466\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "for i in range(0,input_len-seq_length,1):\n",
    "    in_seq=processed_inputs[i:i+seq_length]\n",
    "    out_seq=processed_inputs[i+seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns=len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input sequence to np array and so on\n",
    "X=numpy.reshape(x_data,(n_patterns, seq_length,1))\n",
    "X=X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y=np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "model=Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "filepath='model_weights_saved.hdf5'\n",
    "checkpoint=ModelCheckpoint(filepath, monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompile the model with saved weights\n",
    "filename='model_weights_saved.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the models back into characters\n",
    "num_to_char=dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed :\n",
      "\" ness wonder hope eyes express friend expect informed secret acquainted cannot listen patiently end s \"\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate\n",
    "start=numpy.random.randint(0,len(x_data)-1)\n",
    "pattern=x_data[start]\n",
    "print(\"Random Seed :\")\n",
    "print(\"\\\"\",''.join([num_to_char[value] for value in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trength examined mind seemed passed several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concerning several promise remained secret entered concer"
     ]
    }
   ],
   "source": [
    "# generate the text\n",
    "for i in range(1000):\n",
    "    x=numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x=x/float(vocab_len)\n",
    "    prediction=model.predict(x, verbose=0)\n",
    "    index=numpy.argmax(prediction)\n",
    "    result=num_to_char[index]\n",
    "    seg_in=[num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
